{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "726_project_when_to_interrupt_group_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhfSSoNVmJHR"
      },
      "source": [
        "## Mount the google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7LpMZ3Bt9_q"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "# %cd /gdrive/MyDrive/videos_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzgBtoy83ExM"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjWNJyZmNK0"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO7B70qbemox"
      },
      "source": [
        "%pip install keras-video-generators\r\n",
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNj_1FYUmSOH"
      },
      "source": [
        "## Import required pacakges and their classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNHYhuCklku7"
      },
      "source": [
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\r\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, TimeDistributed, LSTM, Dropout, GRU, Flatten, Input, ConvLSTM2D, BatchNormalization, MaxPool3D, GaussianNoise\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from keras_video import VideoFrameGenerator\r\n",
        "from tensorflow.keras.applications import ResNet50, ResNet50V2\r\n",
        "from kerastuner import RandomSearch, BayesianOptimization\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.utils import plot_model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adZY16uDlomo"
      },
      "source": [
        "### Path for the videos "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieDwjGkclkCz"
      },
      "source": [
        "pattern = '/content/drive/MyDrive/videos_all/{classname}/*.mp4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a4jSefglshm"
      },
      "source": [
        "## Generate video frames "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW4aoZBWlgdj"
      },
      "source": [
        "tr_data = VideoFrameGenerator(\r\n",
        "          classes = ['confused', 'not_confused'],\r\n",
        "          glob_pattern = pattern,\r\n",
        "          nb_frames = 20,\r\n",
        "          split_val = 0.25,\r\n",
        "          shuffle = False,\r\n",
        "          batch_size = 3,\r\n",
        "          target_shape = (224, 224),\r\n",
        "          nb_channel = 3,\r\n",
        "          use_frame_cache = False\r\n",
        ")\r\n",
        "\r\n",
        "va_data = tr_data.get_validation_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul49LC9v_fnT"
      },
      "source": [
        "# tr_data = VideoFrameGenerator(\n",
        "#           classes = ['confused', 'not_confused', 'uncertain'],\n",
        "#           glob_pattern = pattern,\n",
        "#           nb_frames = 20,\n",
        "#           split_val = 0.25,\n",
        "#           shuffle = False,\n",
        "#           batch_size = 3,\n",
        "#           target_shape = (224, 224),\n",
        "#           nb_channel = 3,\n",
        "#           use_frame_cache = False\n",
        "# )\n",
        "\n",
        "# va_data = tr_data.get_validation_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IFaM1d3l40J"
      },
      "source": [
        "## Define Model Structures for 2 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbCLjL6D4HU9"
      },
      "source": [
        "# change the nb fram input from 10 to 20\n",
        "# change the output from 3 to 2\n",
        "# change the decay steps num\n",
        "tr_decay =392\n",
        "def convlstm(hp):\n",
        "    \"\"\"\n",
        "        Network structure extra with ConvLSTM layer.\n",
        "\n",
        "    \"\"\"\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(ConvLSTM2D(filters = 48, kernel_size = (3, 3), return_sequences = True, data_format = \"channels_last\", input_shape = (20,224,224,3)))\n",
        "    model.add(GaussianNoise(0.02))\n",
        "    model.add(ConvLSTM2D(filters = 32, kernel_size = (3, 3), return_sequences = True, data_format = \"channels_last\"))\n",
        "    model.add(GaussianNoise(0.02))\n",
        "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), return_sequences = False, data_format = \"channels_last\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2), data_format='channels_last'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(48, activation = activation))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(24, activation = activation))\n",
        "    model.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = tr_decay/tr_data.batch_size, staircase=True)\n",
        "\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnetlstmgru(hp):\n",
        "    \"\"\"\n",
        "        Network structure extra with ResNet50v2 + LSTM + GRU. \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(ResNet50V2(include_top=False, weights='imagenet', pooling='max'), input_shape=(20,224,224,3)))\n",
        "    # model.add(TimeDistributed(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu')))\n",
        "    # model.add(TimeDistributed(MaxPool2D(pool_size = (2,2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(10, return_sequences = True))\n",
        "    model.add(GRU(10))\n",
        "    model.add(Dense(48, activation = activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(24, activation = activation))\n",
        "    model.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = tr_decay/tr_data.batch_size, staircase=True)\n",
        "\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def cnnlstmgru(hp):\n",
        "    \"\"\"\n",
        "        Network structure with CNN + LSTM + GRU.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = activation), input_shape = (20, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPool2D(pool_size = (2,2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(10, return_sequences = True))\n",
        "    model.add(GRU(10))\n",
        "    model.add(Dense(48, activation = activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(24, activation = activation))\n",
        "    model.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = tr_decay/tr_data.batch_size, staircase=True)\n",
        "\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def cnnlstm(hp):\n",
        "    \"\"\"\n",
        "        Network structure with CNN + LSTM + GRU.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = activation), input_shape = (20, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPool2D(pool_size = (2,2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(10, return_sequences = False))\n",
        "    model.add(Dense(48, activation = activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(24, activation = activation))\n",
        "    model.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = tr_decay/tr_data.batch_size, staircase=True)\n",
        "\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def cnngru(hp):\n",
        "    \"\"\"\n",
        "        Network structure with CNN + LSTM + GRU.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = activation), input_shape = (20, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPool2D(pool_size = (2,2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(GRU(10, return_sequences=False))\n",
        "    model.add(Dense(48, activation = activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(24, activation = activation))\n",
        "    model.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = tr_decay/tr_data.batch_size, staircase=True)\n",
        "\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZKn55J3ZzN6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnb2f7Zu8ix2"
      },
      "source": [
        "## Define Model structure for 3 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ2YB5q2McFO"
      },
      "source": [
        "\r\n",
        "def convlstm(hp):\r\n",
        "    \"\"\"\r\n",
        "        Network structure extra with ConvLSTM layer.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\r\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\r\n",
        "\r\n",
        "    model = Sequential()\r\n",
        "    model.add(ConvLSTM2D(filters = 48, kernel_size = (3, 3), return_sequences = True, data_format = \"channels_last\", input_shape = (10,224,224,3)))\r\n",
        "    model.add(GaussianNoise(0.02))\r\n",
        "    model.add(ConvLSTM2D(filters = 32, kernel_size = (3, 3), return_sequences = True, data_format = \"channels_last\"))\r\n",
        "    model.add(GaussianNoise(0.02))\r\n",
        "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), return_sequences = False, data_format = \"channels_last\"))\r\n",
        "    model.add(MaxPool2D(pool_size=(2,2), data_format='channels_last'))\r\n",
        "    model.add(Dropout(dropout_rate))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(48, activation = activation))\r\n",
        "    model.add(Dropout(0.4))\r\n",
        "    model.add(Dense(24, activation = activation))\r\n",
        "    model.add(Dense(3, activation = 'softmax'))\r\n",
        "\r\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\r\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\r\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = 605/tr_data.batch_size, staircase=True)\r\n",
        "\r\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\r\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnetlstmgru(hp):\r\n",
        "    \"\"\"\r\n",
        "        Network structure extra with ResNet50v2 + LSTM + GRU. \r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\r\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\r\n",
        "\r\n",
        "    model = Sequential()\r\n",
        "    model.add(TimeDistributed(ResNet50V2(include_top=False, weights='imagenet', pooling='max'), input_shape=(10,224,224,3)))\r\n",
        "    # model.add(TimeDistributed(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu')))\r\n",
        "    # model.add(TimeDistributed(MaxPool2D(pool_size = (2,2))))\r\n",
        "    model.add(TimeDistributed(Flatten()))\r\n",
        "    model.add(LSTM(10, return_sequences = True))\r\n",
        "    model.add(GRU(10))\r\n",
        "    model.add(Dense(48, activation = activation))\r\n",
        "    model.add(Dropout(dropout_rate))\r\n",
        "    model.add(Dense(24, activation = activation))\r\n",
        "    model.add(Dense(3, activation = 'softmax'))\r\n",
        "\r\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\r\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\r\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = 605/tr_data.batch_size, staircase=True)\r\n",
        "\r\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\r\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n",
        "def cnnlstmgru(hp):\r\n",
        "    \"\"\"\r\n",
        "        Network structure with CNN + LSTM + GRU.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\r\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\r\n",
        "\r\n",
        "    model = Sequential()\r\n",
        "    model.add(TimeDistributed(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = activation), input_shape = (10, 224, 224, 3)))\r\n",
        "    model.add(TimeDistributed(MaxPool2D(pool_size = (2,2))))\r\n",
        "    model.add(TimeDistributed(Flatten()))\r\n",
        "    model.add(LSTM(10, return_sequences = True))\r\n",
        "    model.add(GRU(10))\r\n",
        "    model.add(Dense(48, activation = activation))\r\n",
        "    model.add(Dropout(dropout_rate))\r\n",
        "    model.add(Dense(24, activation = activation))\r\n",
        "    model.add(Dense(3, activation = 'softmax'))\r\n",
        "\r\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\r\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\r\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = 605/tr_data.batch_size, staircase=True)\r\n",
        "\r\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\r\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def cnnlstm(hp):\r\n",
        "    \"\"\"\r\n",
        "        Network structure with CNN + LSTM + GRU.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\r\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\r\n",
        "\r\n",
        "    model = Sequential()\r\n",
        "    model.add(TimeDistributed(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = activation), input_shape = (10, 224, 224, 3)))\r\n",
        "    model.add(TimeDistributed(MaxPool2D(pool_size = (2,2))))\r\n",
        "    model.add(TimeDistributed(Flatten()))\r\n",
        "    model.add(LSTM(10, return_sequences = False))\r\n",
        "    model.add(Dense(48, activation = activation))\r\n",
        "    model.add(Dropout(dropout_rate))\r\n",
        "    model.add(Dense(24, activation = activation))\r\n",
        "    model.add(Dense(3, activation = 'softmax'))\r\n",
        "\r\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\r\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\r\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = 605/tr_data.batch_size, staircase=True)\r\n",
        "\r\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\r\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def cnngru(hp):\r\n",
        "    \"\"\"\r\n",
        "        Network structure with CNN + LSTM + GRU.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh'])\r\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.9, step=0.2)\r\n",
        "\r\n",
        "    model = Sequential()\r\n",
        "    model.add(TimeDistributed(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = activation), input_shape = (10, 224, 224, 3)))\r\n",
        "    model.add(TimeDistributed(MaxPool2D(pool_size = (2,2))))\r\n",
        "    model.add(TimeDistributed(Flatten()))\r\n",
        "    model.add(GRU(10, return_sequences=False))\r\n",
        "    model.add(Dense(48, activation = activation))\r\n",
        "    model.add(Dropout(dropout_rate))\r\n",
        "    model.add(Dense(24, activation = activation))\r\n",
        "    model.add(Dense(3, activation = 'softmax'))\r\n",
        "\r\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\r\n",
        "    decay_rate = hp.Choice('decay_rate', [0.90, 0.60, 0.30, 0.10])\r\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr, decay_rate=decay_rate, decay_steps = 605/tr_data.batch_size, staircase=True)\r\n",
        "\r\n",
        "    # optimizer = hp.Fixed('optimizer', ['sgd'])\r\n",
        "    model.compile(optimizer=SGD(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8b3EsA2k_cG"
      },
      "source": [
        "## Find the best hyperparameters using RandomSearch\r\n",
        "\r\n",
        "> `RandomSearch(define_model)` : Replace `define_model` parameter with name of the model function you want to train.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ExUe2PVk9-m"
      },
      "source": [
        "# model1: cnngru\r\n",
        "import datetime\r\n",
        "MAX_TRAILS = 66\r\n",
        "epochs = 10\r\n",
        "seed = 40\r\n",
        "\r\n",
        "# change the first parameter according to the network you want to train. \r\n",
        "tuner = RandomSearch(cnngru,\r\n",
        "                     objective='val_accuracy',\r\n",
        "                     max_trials=MAX_TRAILS,\r\n",
        "                     seed=seed,\r\n",
        "                     overwrite=True\r\n",
        "                     )\r\n",
        "\r\n",
        "print(\"Search space summary:\\n {}\".format(tuner.search_space_summary()))\r\n",
        "print(\"Searching starting >>> started\")\r\n",
        "\r\n",
        "tr_decay = 392\r\n",
        "va_decay = 130\r\n",
        "dir = '/content/drive/MyDrive/cnngru/logs/tuner/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "tuner.search(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\r\n",
        "             validation_data=va_data, verbose=1, validation_steps=va_decay//va_data.batch_size, \r\n",
        "             callbacks=[tf.keras.callbacks.TensorBoard(dir)])\r\n",
        "\r\n",
        "print(\"Results summary:\\n {}\".format(tuner.results_summary()))\r\n",
        "print(\"Top 10 best models:\\n {}\".format(tuner.get_best_models(10)))\r\n",
        "\r\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials = 1)[0]\r\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\r\n",
        "\r\n",
        "best_model.save('/content/drive/MyDrive/cnngru/Model/')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3jVa_7XJX-A"
      },
      "source": [
        "# model 2: cnnlstm\n",
        "import datetime\n",
        "MAX_TRAILS = 66\n",
        "epochs = 10\n",
        "seed = 40\n",
        "\n",
        "# change the first parameter according to the network you want to train. \n",
        "tuner = RandomSearch(cnnlstm,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=MAX_TRAILS,\n",
        "                     seed=seed,\n",
        "                     overwrite=True\n",
        "                     )\n",
        "\n",
        "print(\"Search space summary:\\n {}\".format(tuner.search_space_summary()))\n",
        "print(\"Searching starting >>> started\")\n",
        "\n",
        "tr_decay = 392\n",
        "va_decay = 130\n",
        "dir = '/content/drive/MyDrive/cnnlstm/logs/tuner/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tuner.search(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "             validation_data=va_data, verbose=1, validation_steps=va_decay//va_data.batch_size, \n",
        "             callbacks=[tf.keras.callbacks.TensorBoard(dir)])\n",
        "\n",
        "print(\"Results summary:\\n {}\".format(tuner.results_summary()))\n",
        "print(\"Top 10 best models:\\n {}\".format(tuner.get_best_models(10)))\n",
        "\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "\n",
        "best_model.save('/content/drive/MyDrive/cnnlstm/Model/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOMFQ4_AJZPW"
      },
      "source": [
        "# model 3: resnetlstmgru\n",
        "import datetime\n",
        "MAX_TRAILS = 66\n",
        "epochs = 10\n",
        "seed = 40\n",
        "\n",
        "# change the first parameter according to the network you want to train. \n",
        "tuner = RandomSearch(resnetlstmgru,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=MAX_TRAILS,\n",
        "                     seed=seed,\n",
        "                     overwrite=True\n",
        "                     )\n",
        "\n",
        "print(\"Search space summary:\\n {}\".format(tuner.search_space_summary()))\n",
        "print(\"Searching starting >>> started\")\n",
        "\n",
        "tr_decay = 392\n",
        "va_decay = 130\n",
        "dir = '/content/drive/MyDrive/resnetlstmgru/logs/tuner/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tuner.search(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "             validation_data=va_data, verbose=1, validation_steps=va_decay//va_data.batch_size, \n",
        "             callbacks=[tf.keras.callbacks.TensorBoard(dir)])\n",
        "\n",
        "print(\"Results summary:\\n {}\".format(tuner.results_summary()))\n",
        "print(\"Top 10 best models:\\n {}\".format(tuner.get_best_models(10)))\n",
        "\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "\n",
        "best_model.save('/content/drive/MyDrive/resnetlstmgru/Model/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQhex8JBJZYR"
      },
      "source": [
        "# model 4: cnnlstmgru\n",
        "import datetime\n",
        "MAX_TRAILS = 66\n",
        "epochs = 10\n",
        "seed = 40\n",
        "\n",
        "# change the first parameter according to the network you want to train. \n",
        "tuner = RandomSearch(cnnlstmgru,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=MAX_TRAILS,\n",
        "                     seed=seed,\n",
        "                     overwrite=True\n",
        "                     )\n",
        "\n",
        "print(\"Search space summary:\\n {}\".format(tuner.search_space_summary()))\n",
        "print(\"Searching starting >>> started\")\n",
        "\n",
        "tr_decay = 392\n",
        "va_decay = 130\n",
        "dir = '/content/drive/MyDrive/cnnlstmgru/logs/tuner/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tuner.search(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "             validation_data=va_data, verbose=1, validation_steps=va_decay//va_data.batch_size, \n",
        "             callbacks=[tf.keras.callbacks.TensorBoard(dir)])\n",
        "\n",
        "print(\"Results summary:\\n {}\".format(tuner.results_summary()))\n",
        "print(\"Top 10 best models:\\n {}\".format(tuner.get_best_models(10)))\n",
        "\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "\n",
        "best_model.save('/content/drive/MyDrive/cnnlstmgru/Model/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KusHAhVPMM-"
      },
      "source": [
        "# model 5: convlstm\n",
        "# import datetime\n",
        "# MAX_TRAILS = 1\n",
        "# epochs = 10\n",
        "# seed = 40\n",
        "\n",
        "# # change the first parameter according to the network you want to train. \n",
        "# tuner = RandomSearch(convlstm,\n",
        "#                      objective='val_accuracy',\n",
        "#                      max_trials=MAX_TRAILS,\n",
        "#                      seed=seed,\n",
        "#                      overwrite=True\n",
        "#                      )\n",
        "\n",
        "# print(\"Search space summary:\\n {}\".format(tuner.search_space_summary()))\n",
        "# print(\"Searching starting >>> started\")\n",
        "\n",
        "# tr_decay = 392\n",
        "# va_decay = 130\n",
        "# dir = '/content/drive/MyDrive/convlstm/logs/tuner/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tuner.search(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "#              validation_data=va_data, verbose=1, validation_steps=va_decay//va_data.batch_size, \n",
        "#              callbacks=[tf.keras.callbacks.TensorBoard(dir)])\n",
        "\n",
        "# print(\"Results summary:\\n {}\".format(tuner.results_summary()))\n",
        "# print(\"Top 10 best models:\\n {}\".format(tuner.get_best_models(10)))\n",
        "\n",
        "# best_hyperparameters = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "# best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "\n",
        "# best_model.save('/content/drive/MyDrive/convlstm/Model/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF7LUf6pkyv8"
      },
      "source": [
        "## Visualize using Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFHfrAhIiCi4"
      },
      "source": [
        "# %load_ext tensorboard\r\n",
        "# !tensorboard dev upload --logdir /gdrive/MyDrive/logs/fit \\\r\n",
        "#   --name \"When to Interupt\" \\\r\n",
        "#   --description \"Comparing the training results from different neural networks\" \\\r\n",
        "#   --one_shot\r\n",
        "\r\n",
        "# !tensorboard dev list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBoTiBOtkkMk"
      },
      "source": [
        "## Load the trained best model saved before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaorhSZzkaQS"
      },
      "source": [
        "## Train the best model with best hyperparameters from start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKyKyG2eT22w"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "#path = \"/gdrive/MyDrive/Model/\"\n",
        "path = \"/content/drive/MyDrive/cnnlstm/Model/\"\n",
        "cnnlstm_model = load_model(path)\n",
        "cnnlstm_model.summary()\n",
        "plot_model(model, to_file=\"cnnlstm_model.png\", show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"LR\", expand_nested=True, dpi=96)\n",
        "\n",
        "\n",
        "# tr_decay = 392\n",
        "# tr_decay = 130\n",
        "# dir = \"/content/drive/MyDrive/cnnlstm/logs/fit/\"\n",
        "# cnnlstm_model.fit(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "#              validation_data=va_data, verbose=1, validation_steps=tr_decay//va_data.batch_size, \n",
        "#              callbacks=[tf.keras.callbacks.TensorBoard(dir)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xHiATJbUH1j"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "#path = \"/gdrive/MyDrive/Model/\"\n",
        "path = \"/content/drive/MyDrive/cnnlstmgru/Model/\"\n",
        "cnnlstmgru_model = load_model(path)\n",
        "cnnlstmgru_model.summary()\n",
        "plot_model(model, to_file=\"cnnlstmgru_model.png\", show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"LR\", expand_nested=True, dpi=96)\n",
        "\n",
        "tr_decay = 392\n",
        "tr_decay = 130\n",
        "dir = \"/content/drive/MyDrive/cnnlstmgru/logs/fit/\"\n",
        "cnnlstmgru_model.fit(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "             validation_data=va_data, verbose=1, validation_steps=tr_decay//va_data.batch_size, \n",
        "             callbacks=[tf.keras.callbacks.TensorBoard(dir)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN5dq9qlUH_A"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "#path = \"/gdrive/MyDrive/Model/\"\n",
        "path = \"/content/drive/MyDrive/resnetlstmgru/Model/\"\n",
        "resnetlstmgru_model = load_model(path)\n",
        "resnetlstmgru_model.summary()\n",
        "plot_model(model, to_file=\"resnetlstmgru_model.png\", show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"LR\", expand_nested=True, dpi=96)\n",
        "\n",
        "\n",
        "tr_decay = 392\n",
        "tr_decay = 130\n",
        "dir = \"/content/drive/MyDrive/resnetlstmgru/logs/fit/\"\n",
        "resnetlstmgru_model.fit(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "             validation_data=va_data, verbose=1, validation_steps=tr_decay//va_data.batch_size, \n",
        "             callbacks=[tf.keras.callbacks.TensorBoard(dir)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8rRLyM6UILU"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "#path = \"/gdrive/MyDrive/Model/\"\n",
        "path = \"/content/drive/MyDrive/cnngru/Model/\"\n",
        "cnngru_model = load_model(path)\n",
        "cnngru_model.summary()\n",
        "plot_model(model, to_file=\"cnngru_model.png\", show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"LR\", expand_nested=True, dpi=96)\n",
        "\n",
        "\n",
        "tr_decay = 392\n",
        "tr_decay = 130\n",
        "dir = \"/content/drive/MyDrive/cnngru/logs/fit/\"\n",
        "cnngru_model.fit(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "             validation_data=va_data, verbose=1, validation_steps=tr_decay//va_data.batch_size, \n",
        "             callbacks=[tf.keras.callbacks.TensorBoard(dir)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzZ0Eo6HZcs7"
      },
      "source": [
        "# from tensorflow.keras.models import load_model\r\n",
        "# #path = \"/gdrive/MyDrive/Model/\"\r\n",
        "# path = \"/content/drive/MyDrive/cnngru/Model/\"\r\n",
        "# cnngru_model = load_model(path)\r\n",
        "# cnngru_model.summary()\r\n",
        "# plot_model(model, to_file=\"cnngru_model.png\", show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"LR\", expand_nested=True, dpi=96)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFCwQv0M9aQq"
      },
      "source": [
        "# tr_decay = 392\n",
        "# tr_decay = 130\n",
        "# dir = \"/content/drive/MyDrive/cnngru/logs/fit/\"\n",
        "# cnngru_model.fit(tr_data, epochs=epochs, steps_per_epoch=tr_decay//tr_data.batch_size,\n",
        "#              validation_data=va_data, verbose=1, validation_steps=tr_decay//va_data.batch_size, \n",
        "#              callbacks=[tf.keras.callbacks.TensorBoard(dir)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeFb71R2gCv9"
      },
      "source": [
        "# with 3 classes\r\n",
        "\r\n",
        "# dir = \"/gdrive/MyDrive/logs/fit/\"\r\n",
        "# final_model.fit(tr_data, epochs=epochs, steps_per_epoch=605//tr_data.batch_size,\r\n",
        "#              validation_data=va_data, verbose=1, validation_steps=200//va_data.batch_size, \r\n",
        "#              callbacks=[tf.keras.callbacks.TensorBoard(dir)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOstj1NTkT63"
      },
      "source": [
        "## Plot using Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYM4oxktkKoi"
      },
      "source": [
        "!pip install -U tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2mvucOWeNQX"
      },
      "source": [
        "!tensorboard dev upload --logdir /content/drive/MyDrive/cnngru/logs/fit \\\n",
        "  --name \"When to Interupt(cnngru)\" \\\n",
        "  --description \"Comparing the training results from different neural networks\" \\\n",
        "  --one_shot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laDQtXAjeNT-"
      },
      "source": [
        "!tensorboard dev list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqP4YMx9iyvY"
      },
      "source": [
        "# for 3 classes \r\n",
        "\r\n",
        "# !tensorboard dev upload --logdir /gdrive/MyDrive/logs/fit \\\r\n",
        "#   --name \"When to Interupt\" \\\r\n",
        "#   --description \"Comparing the training results from different neural networks\" \\\r\n",
        "#   --one_shot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ36C1pVjFFA"
      },
      "source": [
        "# for 3 classes\r\n",
        "# !tensorboard dev list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJggvgw_hpzS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E4OQDaL9iH8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}